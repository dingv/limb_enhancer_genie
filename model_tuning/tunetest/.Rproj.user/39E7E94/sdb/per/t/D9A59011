{
    "collab_server" : "",
    "contents" : "setwd(\"~/OneDrive/Masters/Publication/model.comparisons_seq/\")\n\n# load(\"Jul03_seq.models_tissues.Rdata\")\n\n# tunedmodels<-c( system(\"ls ./sum_scores_train_not_normalized/Jul.5*2016_seq.models_tissues.Rdata\", intern=TRUE), system(\"ls ./sum_scores_train_not_normalized/Jul.6*2016_seq.models_tissues.Rdata\", intern=TRUE) )\n# tunedmodels<-system(\"ls Jul.7*2016_seq.models_tissues.Rdata\", intern=TRUE)\ntunedmodels<-system(\"ls Jul.11*_seq.models_tissues.Rdata\", intern=TRUE)\n\ntunedmodels<-lapply(tunedmodels, function(x){ load(x); return(tunedmodels.seq)})\ntunedmodels<-do.call(rbind, tunedmodels)\ntunedmodels<-apply(tunedmodels, 2,function(x){do.call(c, x)})\ntunedmodels.seq<-tunedmodels\nrm(tunedmodels)\n\nload(\"160622_clustp_scores_train_scaled.Rdata\")\nload(\"160622_sum_scores_train_normalized_scaled.Rdata\")\n\nvel.act<-read.table(\"velements_meta.txt\")\nall(row.names(vel.act)==row.names(clustp_scores.train))\nall(row.names(vel.act)==row.names(sum_scores.train))\n\nrename<-function(x) sapply( strsplit(x, split=\"_chr\"), '[[', 1 )\n# getting rid of the coordinates...\nrow.names(clustp_scores.train)<-rename(row.names(clustp_scores.train))\nrow.names(sum_scores.train)<-rename(row.names(sum_scores.train))\n\nrequire(tunetest)\n\nrequire(data.table)\nrequire(foreach)\nrequire(doMC)\nrequire(reshape2)\nrequire(stringr)\nrequire(rtracklayer)\nrequire(GenomicRanges)\nrequire(GenomicAlignments)\nrequire(e1071)\nrequire(randomForest)\nrequire(glmnet)\nrequire(caret)\nrequire(PRROC)\nrequire(ROCR)\nrequire(plyr)\nrequire(doMC)\n\n# function that let's us easily access the AUPRC/AUROC values for the predictions\nextr.perf<-function(perflist, type=\"roc\"){\n  \n  if(type==\"roc\") return( lapply( perflist, function(z) {sapply( z, function(y){ sapply( y, function(x){x$roc.curve$auc}) })}) )\n  if(type==\"pr\") return( lapply( perflist, function(z) {sapply( z, function(y){ sapply( y, function(x){x$pr.curve$auc.integral}) })}))\n  \n}\n\n# functions to calculate p-values (Wilcoxon) to see how much the achieved ROCS are better than random\ntest.prediction<-function(x){\n  \n  preds.x<-x[ x[,2]==\"positive\",1]\n  preds.y<-x[ x[,2]==\"negative\",1]\n  \n  wilcox.test(preds.x, preds.y)$p.value\n  \n}\n\ntest.predictionlist_p<-function(perflist){\n  \n  lapply( perflist, function(z) {sapply( z, function(y){ sapply( y, function(x){test.prediction(x$test.pred)}) })}) \n  \n  \n}\n\n# we predict the models on their leave one out test sets (managed internally by the predict.*.fit function)\n# note that the original data used to fit the models has to be loaded and ordered the same for this to work!!!\npredictions.seq<-lapply( tunedmodels.seq, function(model){ lapply(model, function(feature){lapply(feature, function(fold){predict(fold, type=\"dec.values\")} )} )} )\n\n\n# calculation wilcoxon rank tests for the test-oservations:\npredictions_p<-test.predictionlist_p(predictions.seq)\npredictions_p<-melt(predictions_p)\n\n\n# performance boxplots seq:\nperf.seq.roc<-melt( extr.perf( predictions.seq ) )\nggplot(perf.seq.roc, aes(x=L1, col=L1, y=value))+geom_jitter()+geom_boxplot(alpha=0.3)+facet_wrap(~Var2, nrow = 1)+theme_minimal()\n\nperf.seq.pr<-melt( extr.perf( predictions.seq, \"pr\") )\nggplot(perf.seq.pr, aes(x=L1, col=L1, y=value))+geom_jitter()+geom_boxplot(alpha=0.3)+facet_wrap(~Var2, nrow = 1)+theme_minimal()\n\nlibrary(dplyr)\n# comparing the predictions (ROC) with their significance:\nall(perf.seq.roc$Var2==predictions_p$Var2)\nperf.seq.roc$pvalue<-predictions_p$value\nperf.seq.roc$sign.level<-as.numeric(perf.seq.roc$pvalue<=0.05)\nperf.seq.roc$sign.level<-perf.seq.roc$sign.level+(perf.seq.roc$pvalue<=0.01)\nperf.seq.roc$sign.level<-perf.seq.roc$sign.level+(perf.seq.roc$pvalue<=0.001)\nperf.seq.roc$sign.level<-factor(perf.seq.roc$sign.level, labels=c(\"n.s.\",\".05\",\".01\",\".001\"), ordered=TRUE)\nperf.seq.roc$n.obs<-apply(vel.act,2,sum)[match(perf.seq.roc$Var2, colnames(vel.act))]\n\n# tmp<-tapply(perf.seq.roc$value, INDEX = perf.seq.roc$Var2, min)\n# tmp<-data.frame(tmp,label=names(tmp))\n# tmp$x<-perf.seq.roc$n.obs[match(tmp$label, perf.seq.roc$Var2)]\n# \n# set.seed(1)\n# ggplot()+geom_jitter(data = perf.seq.roc, aes(x=n.obs, y=value, col=sign.level, shape=L1))+scale_colour_brewer(palette=\"Blues\")+geom_text(data=tmp, aes(x=x, y=tmp, label=label),angle=90)\n\nperf.seq.roc<-rename_(perf.seq.roc, Model=\"L1\")\nggplot(data = perf.seq.roc)+geom_jitter(aes(x=Model, y=value, col=sign.level, shape=Model))+geom_hline(yintercept=0.5, linetype=2, alpha=0.5)+scale_colour_brewer(palette = \"RdYlGn\")+facet_wrap( n.obs ~ Var2 , scales = \"free_x\")+xlab(\"\")+ylab(\"AUROC\")+theme_minimal()\n\n# tapply(perf.seq.roc$sign.level, INDEX = perf.seq.roc$Model, table)\n# see which models are not complete crap:\nmodel.performancelist<-by(perf.seq.roc$sign.level, INDICES = perf.seq.roc[ c(\"Model\",\"Var2\") ], table)\nmodels_for_genomewide_predictions<-paste(row.names(model.performancelist),rep( colnames(model.performancelist), each=3),sep=\"_\")[ sapply(model.performancelist, function(x){ x[2]>=2 | sum(x[3:4])>=1}) ]\nmodels_for_genomewide_predictions<-data.frame(do.call( rbind, lapply( lapply(models_for_genomewide_predictions,strsplit,\"_\"), '[[', 1)),do.call(rbind, model.performancelist[ sapply(model.performancelist, function(x){ x[2]>=2 | sum(x[3:4])>=1})]))\ncolnames(models_for_genomewide_predictions)<-c(\"Model\",\"Tissue\", \"n.s.\", \".05\", \".01\", \".001\")\nwrite.csv(models_for_genomewide_predictions, \"160713_gw.models_p.values_on_testsets.csv\")\n\n\n# performance barplots sequence:\nperf.seq.roc<-data.table(perf.seq.roc)\nperf.seq.roc_summary<-perf.seq.roc[,list(mean=mean(value),sd=sd(value)),by=c(\"Model\",\"Var2\")]\n\nperf.seq.pr<-data.table(perf.seq.pr)\nperf.seq.pr_summary<-perf.seq.pr[,list(mean=mean(value),sd=sd(value)),by=c(\"L1\",\"Var2\")]\n\nlibrary(scales)\nggplot(perf.seq.roc_summary, aes(x=Var2, y=mean, fill=Model))+geom_bar( stat=\"identity\",position=position_dodge(width=0.8),width=0.7 )+geom_errorbar(aes(ymax=mean+sd,ymin=mean-sd),position=position_dodge(width=0.8),width=0.4)+scale_y_continuous(limits=c(0.5,1), oob=rescale_none)+theme_minimal()+theme(axis.text=element_text(angle=45, hjust=1))\nggplot(perf.seq.pr_summary, aes(x=Var2, y=mean, fill=L1))+geom_bar( stat=\"identity\",position=position_dodge(width=0.8),width=0.7 )+geom_errorbar(aes(ymax=mean+sd,ymin=mean-sd),position=position_dodge(width=0.8),width=0.4)+scale_y_continuous(limits=c(0,1), oob=rescale_none)+theme_minimal()+theme(axis.text=element_text(angle=45, hjust=1))\n\nplot(predictions.seq$svm.lin$limb[[5]], \"pr\")\n\n# coef, seq:\nseq.coefs<-lapply(tunedmodels.seq$lasso, function(x){ do.call(cbind, lapply(x, coef, \"lambda.min\"))})\nsort(apply(as.matrix(seq.coefs$facialmesenchyme[-1,]), 1, mean), decreasing=TRUE)\nlapply(tunedmodels.seq$lasso, function(x){ do.call(cbind, lapply(x, coef, \"lambda.1se\"))})\n\n# importance seq:\nimp.rank<-lapply(tunedmodels.seq$rf, function(x){ do.call(cbind, lapply(x, function(y){rank(importance(y)[,\"positive\"])}))})\nimp.rank<-lapply(tunedmodels.seq$rf, function(x){ do.call(cbind, lapply(x, function(y){importance(y)[,\"positive\"]}))})\n\nimp.rank<-imp.rank$neuraltube\nimp.rank<-imp.rank[ order(apply(imp.rank,1,mean), decreasing=TRUE),]\nlibrary(RColorBrewer)\nhmcol<-colorRampPalette(brewer.pal(9,\"Reds\"))(nrow(imp.rank))\nhmcol<-colorRampPalette(c(rep(\"white\",8),brewer.pal(9,\"Reds\")))(nrow(imp.rank))\nheatmap.2( imp.rank, Rowv = FALSE, Colv=FALSE, dendrogram=\"none\", col = hmcol, trace=\"none\", margins=c(5,18) )\n\n# checking the tuning parameters:\nget.param<-function(x){ x$fit$bestTune }\nparams.seq<-lapply( tunedmodels.seq, function(model){ lapply(model, function(feature){sapply(feature, function(fold){get.param(fold)} )} )} )\n\n\npredictions.seq<-lapply( tunedmodels.seq, function(model){ lapply(model, function(feature){lapply(feature, function(fold){predict(fold, type=\"prob\")} )} )} )\n\n# combining the predictions on the test sets (rank):\nn<-as.vector( lapply( predictions.seq, function(model){ lapply(model, function(feature){do.call(\"rbind\", lapply(feature, function(fold){nrow(fold$test.pred)}) ) } )} )$rf$limb )\npreds.seq<-lapply( predictions.seq, function(model){ lapply(model, function(feature){do.call(\"c\", lapply(feature, function(fold){fold$test.pred[,1]}) ) } )} )\n\ntest.preds.seq<-lapply( names(preds.seq$lasso), function(x){ data.frame(preds.seq$lasso[[ x ]], preds.seq$svm.lin[[ x ]], preds.seq$rf[[ x ]]) })\nnames(test.preds.seq)<-names(tunedmodels.seq$lasso)\n\nfor ( i in 1:length(test.preds.seq) ){\n  \n  colnames(test.preds.seq[[i]])<-c(\"lasso\",\"svm.lin\",\"rf\")\n  test.preds.seq[[i]]$y<-do.call('c', lapply(predictions.seq$lasso[[i]], function(x){x$test.pred[,2]}))\n  test.preds.seq[[i]]$sor<-test.preds.seq[[i]][,1]+test.preds.seq[[i]][,2]+test.preds.seq[[i]][,3]\n  test.preds.seq[[i]]$n<-rep(1:5, n)\n  \n}\n\nsor.perfs<-lapply( test.preds.seq, function(data){\n  \n  pr<-data.table(data)[,lapply(.SD, function(x){pr.curve(x[ y==2 ], x[ y==1 ])$auc.integral}),by=n]\n\n  roc<-data.table(data)[,lapply(.SD, function(x){roc.curve(x[ y==2 ], x[ y==1 ])$auc}),by=n]\n  \n  return(list(data.frame(pr),data.frame(roc)))\n  \n})\n\n\nfor ( i in 1:length(sor.perfs)){\n  par(ask=TRUE)\n  print( ggplot(melt(sor.perfs[[i]][[1]], id.vars=c(\"n\",\"y\")), aes(x=variable, y=value))+geom_line(aes(group=factor(n), col=factor(n)))+geom_point(size=6,col=\"white\")+geom_point(aes(col=factor(n)),size=5)+stat_summary( col=\"black\", alpha=0.5) +ggtitle(label=names(tunedmodels.seq$lasso)[i]))\n  print( ggplot(melt(sor.perfs[[i]][[2]], id.vars=c(\"n\",\"y\")), aes(x=variable, y=value))+geom_line(aes(group=factor(n), col=factor(n)))+geom_point(size=6,col=\"white\")+geom_point(aes(col=factor(n)),size=5)+stat_summary( col=\"black\", alpha=0.5) +ggtitle(label=names(tunedmodels.seq$lasso)[i]))\n  par(ask=FALSE)\n}\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1468017119144.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3561077668",
    "id" : "D9A59011",
    "lastKnownWriteTime" : 1468452546,
    "last_content_update" : 1468452546,
    "path" : "~/OneDrive/Masters/Publication/model.comparisons_seq/Untitled.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}